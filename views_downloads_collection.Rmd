---
title: "views_download_collection"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# loading required libraries
library(tidyverse)
library(osfr)
library(reticulate)
library(jsonlite)
library(lubridate)
library(here)

Sys.setenv(TZ='UTC')

url <- 'https://api.osf.io/_/metrics/preprints/'
osf_auth <- Sys.getenv("osf_preprintimpact_auth")
auth_header <- httr::add_headers('Authorization' = paste('Bearer', osf_auth))

use_condaenv(condaenv = "myenv", conda = "/Users/courtneysoderberg/opt/anaconda3/bin/python")
```

```{r}
# function to create list of dates to pull
pull_dates <- function(start_date){
  dates <- seq(start_date, 
             start_date + days(13), 
             by = 'days')
  
  return(dates)
}

# create tibble with all pps and daily pull windows for each
preprint_pull_dates <- read_csv(here::here('/eligible_preprints.csv')) %>%
                    mutate(dates_to_pull = map(date_published, pull_dates)) %>%
                    unnest(dates_to_pull) %>%
                    rename(gte = dates_to_pull) %>%
                    mutate(lt = gte + days(1),
                           lt_string = gsub(" ", "T", lt),
                           gte_string = gsub(" ", "T", gte))
```

```{python}
pp = r.preprint_pull_dates #get pandas df of R object

import requests
import pandas as pd
import time

METRICS_BASE = r.url
TOKEN = r.osf_auth

headers = {
    'Content-Type': 'application/vnd.api+json',
    'Authorization': 'Bearer {}'.format(TOKEN)
}

post_url = '{}views/'.format(METRICS_BASE)

# set up empty dataframe with the right column names
views_df = pd.DataFrame(columns=['guid', 'sloan_id', 'data_shown', 'date', 'view_count'])
empty_lists = 0
calls = 1

# loop through all the guids that need to be called that day
for i in range(len(pp.guid)):

  # get guid, and start and end days to use in query for each preprint
  lte_date = pp['lt_string'][i]
  gte_date = pp['gte_string'][i]
  guid = pp['guid'][i]
  
  # set up query for getting views per preprint per user_id in timeframe
  query = {
      "query": {
           "term" : { "preprint_id" : guid } # example pp guid
      },
       "aggs" : {
          "views_timeframe": {
              "filter": {
                  "range" : {
                      "timestamp" : {
                          "gte" : gte_date,
                          "lte" : lte_date
                      }
                  }
              },
              "aggs": {
                  "users" : {
                      "terms" : {
                          "field" : "sloan_id",
                          "size" : 5000
                      },
                      "aggs" : {
                      "condition" : {
                        "terms" : {
                          "field" : "sloan_data",
                          "size" : 5000
                        },
                      "aggs": {
                        "views_per_hour" : {
                          "date_histogram" :{
                            "field":"timestamp",
                            "interval":"hour",
                            "format": "yyyy-MM-dd HH:mm:ss"
                          }
                        }
                      }
                  }
              }
          }
      }
  }
}
}
  
  payload = {
      'data': {
          'type': 'preprint_metrics',
          'attributes': {
              'query': query
          }
      }
  }
  
  # make call using query & payload from above
  res = requests.post(post_url, headers=headers, json=payload)
  pp_views_byuser = res.json()['aggregations']['views_timeframe']['users']['buckets']

  calls = calls + 1
  
  if calls % 100 == 0:
    time.sleep(1)
  
  else:
  
    if len(pp_views_byuser) > 0: # only parse query output if there were actually views/downloads to parse
    
      # loop through each person in output
      for x in range(len(pp_views_byuser)):
      
        # for each person, save their dictionary that contatins their actions
        user_actions = pp_views_byuser[x]['condition']
      
        # loop through each item in their 'buckets' which in this case is each timestamp
        for j in range(len(user_actions['buckets'])):
    
          sloan_data_cond = user_actions['buckets'][j]['key_as_string']
          user_views = user_actions['buckets'][j]['views_per_hour']['buckets']
 
          for k in range(len(user_views)):

            # append new row with info for the preprint, the user, the timestamp, and the action count
            new_row = [{'guid':guid,'sloan_id':pp_views_byuser[x]['key'],'data_shown':sloan_data_cond,'date':user_views[k]['key_as_string'],'view_count': user_views[k]['doc_count']}]
    
            views_df = views_df.append(new_row, ignore_index = True)
      
    else:
      empty_lists = empty_lists + 1
```

```{r}
views_by_hour_results <- py$views_df
views_by_hour_results <- views_by_hour_results %>%
                           as_tibble() %>%
                            unnest(view_count)

write_csv(views_by_hour_results, 'views_by_hour_results.csv')
```

```{python}
pp = r.preprint_pull_dates #get pandas df of R object

import requests
import pandas as pd
import time

METRICS_BASE = r.url
TOKEN = r.osf_auth

headers = {
    'Content-Type': 'application/vnd.api+json',
    'Authorization': 'Bearer {}'.format(TOKEN)
}

post_url = '{}downloads/'.format(METRICS_BASE)

# set up empty dataframe with the right column names
downloads_df = pd.DataFrame(columns=['guid', 'sloan_id', 'data_shown', 'date', 'download_count'])
empty_lists = 0
calls = 1

# loop through all the guids that need to be called that day
for i in range(len(pp.guid)):

  # get guid, and start and end days to use in query for each preprint
  lte_date = pp['lt_string'][i]
  gte_date = pp['gte_string'][i]
  guid = pp['guid'][i]
  
  # set up query for getting downloads per preprint per user_id in timeframe
  query = {
      "query": {
           "term" : { "preprint_id" : guid } # example pp guid
      },
       "aggs" : {
          "downloads_timeframe": {
              "filter": {
                  "range" : {
                      "timestamp" : {
                          "gte" : gte_date,
                          "lte" : lte_date
                      }
                  }
              },
              "aggs": {
                  "users" : {
                      "terms" : {
                          "field" : "sloan_id",
                          "size" : 5000
                      },
                      "aggs" : {
                      "condition" : {
                        "terms" : {
                          "field" : "sloan_data",
                          "size" : 5000
                        },
                      "aggs": {
                        "downloads_per_hour" : {
                          "date_histogram" :{
                            "field":"timestamp",
                            "interval":"hour",
                            "format": "yyyy-MM-dd HH:mm:ss"
                          }
                        }
                      }
                  }
              }
          }
      }
  }
}
}
  
  payload = {
      'data': {
          'type': 'preprint_metrics',
          'attributes': {
              'query': query
          }
      }
  }
  
  # make call using query & payload from above
  res = requests.post(post_url, headers=headers, json=payload)
  pp_downloads_byuser = res.json()['aggregations']['downloads_timeframe']['users']['buckets']

  calls = calls + 1
  
  if calls % 100 == 0:
    time.sleep(1)
  
  else:
  
    if len(pp_downloads_byuser) > 0: # only parse query output if there were actually views/downloads to parse
    
      # loop through each person in output
      for x in range(len(pp_downloads_byuser)):
      
        # for each person, save their dictionary that contatins their actions
        user_actions = pp_downloads_byuser[x]['condition']
      
        # loop through each item in their 'buckets' which in this case is each timestamp
        for j in range(len(user_actions['buckets'])):
    
          sloan_data_cond = user_actions['buckets'][j]['key_as_string']
          user_downloads = user_actions['buckets'][j]['downloads_per_hour']['buckets']
 
          for k in range(len(user_downloads)):

            # append new row with info for the preprint, the user, the timestamp, and the action count
            new_row = [{'guid':guid,'sloan_id':pp_downloads_byuser[x]['key'],'data_shown':sloan_data_cond,'date':user_downloads[k]['key_as_string'],'download_count': user_downloads[k]['doc_count']}]
    
            downloads_df = downloads_df.append(new_row, ignore_index = True)
      
    else:
      empty_lists = empty_lists + 1
```



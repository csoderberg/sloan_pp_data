---
title: "views_download_collection"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# loading required libraries
library(tidyverse)
library(osfr)
library(reticulate)
library(jsonlite)
library(lubridate)
library(here)

Sys.setenv(TZ='UTC')

url <- 'https://api.osf.io/_/metrics/preprints/'
osf_auth <- Sys.getenv("osf_preprintimpact_auth")
auth_header <- httr::add_headers('Authorization' = paste('Bearer', osf_auth))

use_condaenv(condaenv = "myenv", conda = "/Users/courtneysoderberg/opt/anaconda3/bin/python")
```

```{r}
# function to create list of dates to pull
pull_dates <- function(start_date){
  dates <- seq(start_date, 
             start_date + days(13), 
             by = 'days')
  
  return(dates)
}

# create tibble with all pps and daily pull windows for each
preprint_pull_dates <- read_csv(here::here('/eligible_preprints.csv')) %>%
                    mutate(dates_to_pull = map(date_published, pull_dates)) %>%
                    unnest(dates_to_pull) %>%
                    rename(gte = dates_to_pull) %>%
                    mutate(lt = gte + days(1),
                           lt_string = gsub(" ", "T", lt),
                           gte_string = gsub(" ", "T", gte))
```

```{python}
pp = r.preprint_pull_dates[0:10] #get pandas df of R object

import requests
import pandas as pd

METRICS_BASE = r.url
TOKEN = r.osf_auth

headers = {
    'Content-Type': 'application/vnd.api+json',
    'Authorization': 'Bearer {}'.format(TOKEN)
}

post_url = '{}views/'.format(METRICS_BASE)

# set up empty dataframe with the right column names
views_df = pd.DataFrame(columns=['guid', 'user_id', 'date', 'view_count'])
empty_lists = 0

# loop through all the guids that need to be called that day
for i in range(len(pp.guid)):

  # get guid, and start and end days to use in query for each preprint
  lte_date = pp['lt_string'][i]
  gte_date = pp['gte_string'][i]
  guid = pp['guid'][i]
  
  # set up query for getting views per preprint per user_id in timeframe
  query = {
      "query": {
           "term" : { "preprint_id" : guid } # example pp guid
      },
       "aggs" : {
          "views_timeframe": {
              "filter": {
                  "range" : {
                      "timestamp" : {
                          "gte" : gte_date,
                          "lte" : lte_date
                      }
                  }
              },
              "aggs": {
                  "users" : {
                      "terms" : {
                          "field" : "user_id",
                          "size" : 5000
                      },
                      "aggs": {
                        "views_per_day" : {
                          "date_histogram" :{
                            "field":"timestamp",
                            "interval":"minute",
                            "format": "yyyy-MM-dd HH:mm:ss"
                          }
                        }
                      }
                  }
              }
          }
      }
  }
  
  
  payload = {
      'data': {
          'type': 'preprint_metrics',
          'attributes': {
              'query': query
          }
      }
  }
  
  # make call using query & payload from above
  res = requests.post(post_url, headers=headers, json=payload)
  pp_views_byuser = res.json()['aggregations']['views_timeframe']['users']['buckets']


  if len(pp_views_byuser) > 0: # only parse query output if there were actually views/downloads to parse
    
    # loop through each person in output
    for x in range(len(pp_views_byuser)):
      
      # for each person, save their dictionary that contatins their actions
      user_actions = pp_views_byuser[x][key]
      
      # loop through each item in their 'buckets' which in this case is each timestamp
      for j in range(len(user_actions['buckets'])):
        
        # append new row with info for the preprint, the user, the timestamp, and the action count
        views_df = views_df.append({'guid': guid, 'user_id': pp_views_byuser[x]['key'], 'date': user_actions['buckets'][j]['key_as_string'], 'view_count': user_actions['buckets'][j]['doc_count']}, ignore_index= True)
  else:
    empty_lists = empty_lists + 1
```

